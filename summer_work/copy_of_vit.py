# -*- coding: utf-8 -*-
"""Copy of VIT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1v61JiwiivwCsdglK_cA7lpY1_kBRA5Nt

# Step 1 : Importing the Necessary Libraries
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch.nn import init
from torch.nn.parameter import Parameter
import torchvision
from tqdm import tqdm
from torch.utils.data import Dataset, DataLoader, Subset
from torchvision import transforms, models
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
try:
  from einops import rearrange
except ModuleNotFoundError:
  !pip install einops
  from einops import rearrange
try:
  from timm.models.layers import DropPath, trunc_normal_, to_2tuple
except:
  !pip install timm
  from timm.layers.helpers import to_2tuple
  from timm.layers.drop import DropPath

"""Let's consider an input image with dimensions 3x224x224, where:
3 is the number of channels (RGB),
224x224 is the height and width of the image.

Input Shape: 1 (batch size) x 3 (channels) x 224 (height) x 224 (width)

Output Shape (after PatchEmbed):
num_patches = (224 / 16) * (224 / 16) = 196 patches

Each patch is embedded into a vector of size embed_dim (default 768).

Output Shape: 1 (batch size) x 196 (num_patches) x 768 (embed_dim)
"""

image_size = 224
patch_size = 16
embed_dim = 768
to_2tuple(image_size)

"""x.flatten(2) flattens the tensor starting from the second dimension, combining the height and width dimensions into a single dimension. So, it changes the shape from [B, 768, 14, 14] to [B, 768, 196]."""

class PatchEmbed(nn.Module):
    """ Image to Patch Embedding
      Summary:
        1. We consider an image of size 224 x 224 and having 3 channels
        2. Our patch size is taken to be as 16
        3. Consider 3 channels RGB
        4. embed_dim : 768 (16 x 16 x 3) # desired output channel shape
        5. We have 196 patches in the image as (num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0]))
      Returns:
        Output Shape: 1 (batch size) x 196 (num_patches) x 768 (embed_dim)
    """
    def __init__(self, img_size=224, patch_size=16, in_chans=3, embed_dim=768):
        super().__init__()
        img_size = to_2tuple(img_size) # tuple of (224,224)
        # print(img_size)
        patch_size = to_2tuple(patch_size) # tuple of (16,16)
        # print(patch_size)
        num_patches = (img_size[1] // patch_size[1]) * (img_size[0] // patch_size[0])
        self.img_size = img_size
        self.patch_size = patch_size
        self.num_patches = num_patches

        self.proj = nn.Conv2d(in_chans, embed_dim, kernel_size=patch_size, stride=patch_size)

    def forward(self, x):
        B, C, H, W = x.shape
        assert H == self.img_size[0] and W == self.img_size[1], \
            f"Input image size ({H}*{W}) doesn't match model ({self.img_size[0]}*{self.img_size[1]})."
        x = self.proj(x)
        print(x.shape) # torch.Size([1, 768, 14, 14])
        x = x.flatten(2)
        print(x.shape) # torch.Size([1, 768, 196])
        x = x.transpose(1, 2)
        print(x.shape) # torch.Size([1, 196, 768])
        return x

x = torch.randn(1, 3, 224, 224)
patch_embed = PatchEmbed()
patch_embed(x).shape # torch.Size([1, 196, 768])

class Mlp(nn.Module):
    """ MLP as used in Vision Transformer,
      Returns:
        Output Shape: 1 (batch size) x 196 (num_patches) x 768 (embed_dim)
    """
    def __init__(self, in_features, hidden_features=None, out_features=None, act_layer=nn.GELU, drop=0.):
        super().__init__()
        out_features = out_features or in_features
        hidden_features = hidden_features or in_features
        self.fc1 = nn.Linear(in_features, hidden_features)
        self.act = act_layer()
        self.fc2 = nn.Linear(hidden_features, out_features)
        self.drop = nn.Dropout(drop)

    def forward(self, x):
        """
        Forward pass for the MLP module.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, num_patches, embed_dim).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, num_patches, embed_dim).
        """
        x = self.fc1(x)
        print(f"[Mlp] After fc1 shape: {x.shape}")  # Tagged print statement

        x = self.act(x)
        x = self.drop(x)
        x = self.fc2(x)
        print(f"[Mlp] After fc2 shape: {x.shape}")  # Tagged print statement

        x = self.drop(x)
        return x

x = torch.randn(1, 196, 768) # from path embeding

# Create an instance of Mlp
mlp = Mlp(in_features=768, hidden_features=1024, out_features=512, drop=0.1)

# Pass the tensor through the Mlp instance and print the output shape
output = mlp(x)
print(output.shape)

class Attention(nn.Module):
    """
    Multi-Head Self Attention Module.

    Args:
        dim (int): Dimension of input features.
        num_heads (int): Number of attention heads. Default is 8.
        qkv_bias (bool): If True, add bias to the linear layers for Q, K, V. Default is False.
        qk_scale (float): Custom scale for QK. Default is None (uses 1/sqrt(head_dim)).
        attn_drop (float): Dropout rate for attention weights. Default is 0.0.
        proj_drop (float): Dropout rate for output projection. Default is 0.0.
    """
    def __init__(self, dim, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):
        super().__init__()
        self.num_heads = num_heads
        head_dim = dim // num_heads  # Dimension per head
        self.scale = qk_scale or head_dim ** -0.5  # Scaling factor for QK (will be 1/sqrt(head_dim))

        # Linear layer to project input into queries, keys, and values
        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)  # Linear layer for Q, K, V
        self.attn_drop = nn.Dropout(attn_drop)  # Dropout for attention weights
        self.proj = nn.Linear(dim, dim)  # Output projection layer
        self.proj_drop = nn.Dropout(proj_drop)  # Dropout for output projection

    def forward(self, x):
        """
        Forward pass for the attention module.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, num_tokens, dim).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, num_tokens, dim).
        """
        B, N, C = x.shape  # Batch size, number of patches, and feature dimension
        print(f"[Attention] Input shape: {x.shape}")  # Tagged print statement

        # Project input to Q, K, V and reshape
        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)
        print(f"[Attention] QKV shape: {qkv.shape}")  # Tagged print statement

        # Split into queries, keys, and values
        q, k, v = qkv[0], qkv[1], qkv[2]

        # Calculate scaled dot-product attention
        attn = (q @ k.transpose(-2, -1)) * self.scale
        attn = attn.softmax(dim=-1)
        attn = self.attn_drop(attn)

        # Apply attention to values and reshape
        x = (attn @ v).transpose(1, 2).reshape(B, N, C)

        # Project the output and apply dropout
        x = self.proj(x)
        x = self.proj_drop(x)
        return x

import torch

# Create an input tensor with shape [1, 196, 768]
x = torch.randn(1, 196, 768)

# Create an instance of Attention
attention = Attention(dim=768, num_heads=12, qkv_bias=True, attn_drop=0.1, proj_drop=0.1)

# Pass the tensor through the Attention instance and print the output shape
output = attention.forward(x)
print(output.shape)  # Expected output shape: [1, 196, 768]

class Block(nn.Module):
    """
    Transformer Block consisting of Multi-Head Self Attention and MLP layers.

    Args:
        dim (int): Dimension of input features.
        num_heads (int): Number of attention heads.
        mlp_ratio (float): Ratio of hidden units in MLP to input dimension. Default is 4.0.
        qkv_bias (bool): If True, add bias to the linear layers for Q, K, V. Default is False.
        qk_scale (float): Custom scale for QK. Default is None (uses 1/sqrt(head_dim)).
        drop (float): Dropout rate for both attention and MLP layers. Default is 0.0.
        attn_drop (float): Dropout rate for attention weights. Default is 0.0.
        drop_path (float): Dropout rate for stochastic depth (DropPath). Default is 0.0.
        act_layer (torch.nn.Module): Activation function. Default is nn.GELU.
        norm_layer (torch.nn.Module): Normalization layer. Default is nn.LayerNorm.
    """
    def __init__(self, dim, num_heads, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop=0., attn_drop=0.,
                 drop_path=0., act_layer=nn.GELU, norm_layer=nn.LayerNorm):
        super().__init__()
        self.norm1 = norm_layer(dim)
        self.attn = Attention(
            dim, num_heads=num_heads, qkv_bias=qkv_bias, qk_scale=qk_scale, attn_drop=attn_drop, proj_drop=drop)
        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()  # DropPath for stochastic depth
        self.norm2 = norm_layer(dim)
        mlp_hidden_dim = int(dim * mlp_ratio)
        self.mlp = Mlp(in_features=dim, hidden_features=mlp_hidden_dim, act_layer=act_layer, drop=drop)

    def forward(self, x):
        """
        Forward pass for the Transformer Block.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, num_patches, dim).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, num_patches, dim).
        """
        # Self-attention and normalization
        x = x + self.drop_path(self.attn(self.norm1(x)))

        # MLP and normalization
        x = x + self.drop_path(self.mlp(self.norm2(x)))
        return x


# Testing code to check the output shapes
if __name__ == "__main__":
    import torch

    # Example usage
    batch_size = 1
    num_patches = 196
    embed_dim = 768
    dim = embed_dim
    num_heads = 12

    # Create an instance of the Block
    block = Block(dim=dim, num_heads=num_heads)

    # Generate a dummy input tensor
    x = torch.randn(batch_size, num_patches, dim)

    # Forward pass through the Block
    output = block(x)

    # Print output shape for verification
    print(f"Output shape: {output.shape}")

"""Inside VisionTransformer class initialization:

self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))

Output Shape (after adding positional embedding):
`1 (batch size) x 197 (num_patches + 1 for cls_token) x 768 (embed_dim)`
"""

class VisionTransformer(nn.Module):
    """ Vision Transformer with support for patch or hybrid CNN input stage.

    Args:
        img_size (int): Input image size. Default is 224.
        patch_size (int): Patch size for patch embeddings. Default is 16.
        in_chans (int): Number of input channels. Default is 3 (RGB images).
        num_classes (int): Number of output classes. Default is 1000.
        embed_dim (int): Dimensionality of the token embeddings. Default is 768.
        depth (int): Number of encoder blocks. Default is 12.
        num_heads (int): Number of attention heads. Default is 12.
        mlp_ratio (float): Ratio of MLP hidden units to embed_dim. Default is 4.0.
        qkv_bias (bool): If True, add bias to the QKV linear layers. Default is False.
        qk_scale (float): Scale factor for QK if not provided, computed as 1/sqrt(head_dim).
        drop_rate (float): Dropout rate for all layers. Default is 0.0.
        attn_drop_rate (float): Dropout rate for attention weights. Default is 0.0.
        drop_path_rate (float): Dropout rate for stochastic depth. Default is 0.0.
        norm_layer (torch.nn.Module): Normalization layer. Default is nn.LayerNorm.
    """

    def __init__(self, img_size=224, patch_size=16, in_chans=3, num_classes=1000, embed_dim=768, depth=12,
                 num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., attn_drop_rate=0.,
                 drop_path_rate=0., norm_layer=nn.LayerNorm):
        super().__init__()
        self.num_classes = num_classes
        self.num_features = self.embed_dim = embed_dim  # num_features for consistency with other models

        # Patch Embedding
        self.patch_embed = PatchEmbed(
            img_size=img_size, patch_size=patch_size, in_chans=in_chans, embed_dim=embed_dim)
        num_patches = self.patch_embed.num_patches

        # Class token and positional embeddings
        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))
        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))
        self.pos_drop = nn.Dropout(p=drop_rate)

        # Stochastic depth decay rule
        # torch.linspace generates a 1-dimensional tensor of depth equally spaced values between 0 and drop_path_rate, inclusive.
        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]

        # Encoder Blocks (12)
        self.blocks = nn.ModuleList([
            Block(
                dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, qk_scale=qk_scale,
                drop=drop_rate, attn_drop=attn_drop_rate, drop_path=dpr[i], norm_layer=norm_layer)
            for i in range(depth)])

        # Final Layer Normalization
        self.norm = norm_layer(embed_dim)

        # Classifier head
        """self.head:
              Output layer for classification tasks, mapping the final transformer output to class probabilities or
              feature representations based on num_classes"""
        self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()

    def forward_features(self, x):
        """
        Forward pass through the feature extraction part of the Vision Transformer.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_chans, img_size, img_size).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, embed_dim).
        """
        B = x.shape[0]

        # Patch Embedding
        x = self.patch_embed(x)

        # Adding class token and positional embeddings
        cls_tokens = self.cls_token.expand(B, -1, -1)
        x = torch.cat((cls_tokens, x), dim=1)  # Concatenate class token and patch embeddings
        x = x + self.pos_embed  # adds the positional embeddings
        x = self.pos_drop(x)

        # Transformer Blocks
        for blk in self.blocks:
            x = blk(x)

        # Final Layer Normalization
        x = self.norm(x)

        # Return only the class token
        return x[:, 0]

    def forward(self, x):
        """
        Forward pass through the entire Vision Transformer.

        Args:
            x (torch.Tensor): Input tensor of shape (batch_size, in_chans, img_size, img_size).

        Returns:
            torch.Tensor: Output tensor of shape (batch_size, num_classes) if num_classes > 0,
                        otherwise (batch_size, embed_dim).
        """
        # Feature extraction through the Vision Transformer
        x = self.forward_features(x)

        # Classification head
        x = self.head(x)

        return x

# test for random image
random_image = torch.randn(1, 3, 224, 224).to(device)
print(random_image.shape)
vit = VisionTransformer()
vit.to(device)
output = vit(random_image)
output.shape

# Hyperparameters
batch_size = 8
learning_rate = 0.001
num_epochs = 5


# Data augmentation and normalization for training
transform_train = transforms.Compose([
    transforms.RandomResizedCrop(224),  # Resize the images to 224x224
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

# Normalization for validation
transform_test = transforms.Compose([
    transforms.Resize(224),
    transforms.ToTensor(),
    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),
])

train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)
test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)

train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1)
test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=1)

# Calculate indices for 10% of the dataset
import numpy as np

train_size = int(0.1 * len(train_dataset))
test_size = int(0.1 * len(test_dataset))
train_indices = np.random.choice(len(train_dataset), train_size, replace=False)
test_indices = np.random.choice(len(test_dataset), test_size, replace=False)

# Create subsets
train_subset = Subset(train_dataset, train_indices)
test_subset = Subset(test_dataset, test_indices)

train_loader_subset = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=1)
test_loader_subset = DataLoader(test_subset, batch_size=batch_size, shuffle=False, num_workers=1)

train_loader.dataset.classes

# Initialize the model
vit = VisionTransformer(num_classes=10).to(device)

# Loss and optimizer
criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(vit.parameters(), lr=learning_rate)

# Training function
def train_model(model, train_loader, criterion, optimizer, num_epochs, device):
    model.train()
    for epoch in tqdm(range(num_epochs)):
        running_loss = 0.0
        for images, labels in tqdm(train_loader):
            images, labels = images.to(device), labels.to(device)
            # print(images.shape)

            # Forward pass
            outputs = model(images)
            loss = criterion(outputs, labels)

            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            running_loss += loss.item()

        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {running_loss / len(train_loader):.4f}')

# Testing function
def test_model(model, test_loader, device):
    model.eval()
    correct = 0
    total = 0
    with torch.no_grad():
        for images, labels in (test_loader):
            images, labels = images.to(device), labels.to(device)
            outputs = model(images)
            _, predicted = torch.max(outputs.data, 1)
            total += labels.size(0)
            correct += (predicted == labels).sum().item()

    print(f'Accuracy of the model on the test images: {100 * correct / total:.2f}%')

# Train the model
num_epochs = 3
train_model(vit, train_loader_subset, criterion, optimizer, num_epochs , device)

# Test the model
test_model(vit, test_loader_subset, device)



