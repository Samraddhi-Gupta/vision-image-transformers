{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Samraddhi-Gupta/vision-image-transformers/blob/main/Text_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Feature Engineering\n",
        "\n",
        "Before feeding any ML model some kind data, it has to be properly preprocessed. You must have heard the byword: Garbage in, garbage out (GIGO). Text is a specific kind of data and can't be directly fed to most ML models, so before feeding it to a model you have to somehow extract numerical features from it, in other word vectorize. Vectorization is not the topic of this tutorial, but the main thing you have to understand is that GIGO is also aplicable on vectorization too, you can extract qualitative features only from qualitatively preprocessed text.\n",
        "Contents:\n",
        "\n",
        "* Extracting Basic Features\n",
        "    - Number of Characters\n",
        "    - Number of Words\n",
        "    - Average Word Length\n",
        "    - Number of Hastags and Mentions (Social Media)\n",
        "    - Number of Sentences\n",
        "    - Number of Paragraphs\n",
        "    - Words Starting with an Uppercase\n",
        "    - All-capital Words\n",
        "    - Numeric quantities\n",
        "* Text Preprocessing\n",
        "    - Tokenization/Segmentation\n",
        "    - Lemmatization\n",
        "    - Stemming\n",
        "    - Converting to Lowercase\n",
        "    - Text Cleaning\n",
        "        - Removing Unnecessary Whitespaces and Escape Squences\n",
        "        - Removing Punctuations\n",
        "        - Removing Stopwords or Commonly Occuring Words/Tokens\n",
        "        - Removing Special Characters (emojis, numbers...)\n",
        "        - Expanding Contractions (don't, etc.)\n",
        "* Extracting Word Features\n",
        "    - Parts-of-Speech (POS) Tagging\n",
        "    - Named Entity Recognition (NEG)\n",
        "* Dependency Parsing (Not in the Notebook)\n",
        "* Vectorization (Convert documents into a set of numerical features)\n",
        "    - Bag of Words (Bag of n-grams)\n",
        "    - tf-idf\n",
        "\n",
        "_Dataset: [troll-tweets](https://github.com/fivethirtyeight/russian-troll-tweets)_"
      ],
      "metadata": {
        "id": "lDpJQNfkVitA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "grucNPFubIEw",
        "outputId": "1da06740-fe99-41cf-a708-ef0c1ff8e118"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NCW_ExcSMfov"
      },
      "outputs": [],
      "source": [
        "# Import necessary modules\n",
        "\n",
        "import re\n",
        "\n",
        "import nltk\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('wordnet')\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('averaged_perceptron_tagger') # NER\n",
        "# nltk.download('maxent_ne_chunker') # NER\n",
        "# nltk.download('words') # NER\n",
        "# nltk.download('tagsets') #NER to see the tags available\n",
        "\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from nltk.tokenize import regexp_tokenize, TweetTokenizer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from gensim.models.tfidfmodel import TfidfModel\n",
        "\n",
        "import spacy\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "from collections import Counter, defaultdict\n",
        "\n",
        "from itertools import chain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preparing Text for NLP"
      ],
      "metadata": {
        "id": "ADghvpQ5bOtx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Regex\n",
        "\n",
        "A _Regular Expression_ is a sequence of characters or a string containing a combination of normal (or regular, ordinary) characters and special metacharacters that define search patterns to find text or positions within a text. While normal characters have literal meaning and match themselves, metacharacters have special meaning and they resresent types of characters, such as `\\d` for digits or `\\w` for words, or ideas/repetitions.\n",
        "\n",
        ">Regular expressions use the backslash character ('\\') to indicate special forms or to allow special characters to be used without invoking their special meaning. This collides with Python’s usage of the same character for the same purpose in string literals. The solution is to use Python’s raw string notation for regular expression patterns; backslashes are not handled in any special way in a string literal prefixed with `'r'`. So `r\"\\n\"` is a two-character string containing `'\\'` and `'n'`, while `\"\\n\"` is a one-character string containing a newline. Usually patterns will be expressed in Python code using this raw string notation. [source](https://docs.python.org/3/library/re.html)\n",
        "\n",
        "Some of the special characters are shown in below tables.\n",
        "\n",
        "Metacharacter|Description\n",
        "---|:---\n",
        "`.` | Wildcard - Matches any single character except \\n.Matches any single character except `\\n`\n",
        "`^` | Matches the start of the string\n",
        "`$` | Matches the end of the string or just before the newline at the end of the string\n",
        "`\\|` | The OR (choice) operator matches either the expression before or the expression after the operator. For example, `abc\\|def` matches \"abc\" or \"def\".\n",
        "\n",
        "**Repetition Quantifiers:**\n",
        "\n",
        "Metacharacter|Description\n",
        "---|:---\n",
        "`*` | Matches the preceding element 0 or more times. `ab*` will match ‘a’, ‘ab’, or ‘a’ followed by any number of ‘b’s\n",
        "`+` | Matches the preceding element 1 or more times. `ab+` will match ‘a’ followed by any non-zero number of ‘b’s; it will not match just ‘a’\n",
        "`?` | Causes the resulting RE to match 0 or 1 repetitions of the preceding RE. `ab?` will match either ‘a’ or ‘ab’.\n",
        "`{m,n}` | Matches the preceding element at least m and not more than n times. For example, `a{3,5}` | Matches only \"aaa\", \"aaaa\", and \"aaaaa\"\n",
        "\n",
        "**Special Sequences:**\n",
        "\n",
        "Metacharacter|Description\n",
        "---|:---\n",
        "`\\w` | Matches alphanumeric characters, which means a-z, A-Z, and 0-9. It also matches the underscore, _.\n",
        "`\\d` | Matches digits, which means 0-9.\n",
        "`\\s` | Matches whitespace characters, which include the \\t, \\n, \\r, and space characters.\n",
        "`\\b` | Matches the boundary (or empty string) at the start and end of a word, that is, between \\w and \\W.\n",
        "`\\A` | Matches the beginning of a string (but not an internal line).\n",
        "`\\z` | Matches the end of a string (but not an internal line).\n",
        "\n",
        "**Ranges (Set of Characters):**\n",
        "\n",
        "Metacharacter|Description\n",
        "---|:---\n",
        "`[ ]` | Used to indicate a set of characters. Matches a single character that is contained within the brackets\n",
        "`[amk]` | Matches either a, m, or k. It does not match `amk`\n",
        "`[a-z]` | Matches any alphabet from `a` to `z`\n",
        "`[a\\-z]` | Matches a, -, or z. It matches `-` because `\\` escapes it\n",
        "`[a-cx-z]` | Matches a, b, c, x, y, z\n",
        "`[0-5][0-9]` | Matches all the two-digits numbers from 00 to 59\n",
        "`[a-]` | Matches a or -, because `-` is not being used to indicate a series of characters\n",
        "`[-a]` | As above, matches a or -\n",
        "`[a-z0-9]` | Matches characters from a to z and also from 0 to 9\n",
        "`[(+*)]` | Special characters become literal inside a set, so this matches (, +, *, and )\n",
        "`[^ab2]` | Adding ^ excludes any character in the set. Here, it matches characters that are not a, b, or 2\n",
        "\n",
        "**Groups:**\n",
        "\n",
        "Metacharacter|Description\n",
        "---|:---\n",
        "`( )` | Matches the expression inside the parentheses and groups it. Groups a series of pattern elements to a single element.\n",
        "`(? )` | Inside parentheses like this, ? acts as an extension notation. Its meaning depends on the character immediately to its right."
      ],
      "metadata": {
        "id": "cYONyhSXbU1i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_text = '<h2>HTML Element</h2><p>The HTML <code class=\"w3-codespan\">&#x8a;sup&#x3e;</code> element defines superscript text. Superscript text appears half a character above the normal line, and is sometimes rendered in a smaller font. Superscript text can be used for footnotes, like WWW<sup>[1]</sup> up>:</p>'"
      ],
      "metadata": {
        "id": "l8olgpaXbKN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "extracted_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "yIv7HeEPcULg",
        "outputId": "06327de7-76b5-47e6-f860-00718b2e3696"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'<h2>HTML Element</h2><p>The HTML <code class=\"w3-codespan\">&#x8a;sup&#x3e;</code> element defines superscript text. Superscript text appears half a character above the normal line, and is sometimes rendered in a smaller font. Superscript text can be used for footnotes, like WWW<sup>[1]</sup> up>:</p>'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "def striphtml(data):\n",
        "    p = re.compile(r'<.*?>')\n",
        "    return p.sub('', data)"
      ],
      "metadata": {
        "id": "QCRlKgE5cWT_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "striphtml(extracted_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "Tc6hzk0ZceCo",
        "outputId": "548f28d6-047d-42e7-ff5b-50518afbac8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'HTML ElementThe HTML &#x8a;sup&#x3e; element defines superscript text. Superscript text appears half a character above the normal line, and is sometimes rendered in a smaller font. Superscript text can be used for footnotes, like WWW[1] up>:'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Unicode Normalization"
      ],
      "metadata": {
        "id": "cbw4ucuJdT0A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emoji_text = \"\"\"First, we established subjects. I was 🙂, and I could use it to represent \"me\" in sentences. For example, 🙂🚶‍♀️🏃‍♂️ could mean \"I'm running to work.\" \"\"\""
      ],
      "metadata": {
        "id": "71R1jFgEdKSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "emoji_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "id": "HATJDrGLcpHz",
        "outputId": "1386e2df-e2c5-4405-d150-8c3aaadf0b70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'First, we established subjects. I was 🙂, and I could use it to represent \"me\" in sentences. For example, 🙂🚶\\u200d♀️🏃\\u200d♂️ could mean \"I\\'m running to work.\" '"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "emoji_text.encode('utf-8')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dRF53x0FdspT",
        "outputId": "d8ac5941-1ac9-457e-a8be-eaa4675a178c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "b'First, we established subjects. I was \\xf0\\x9f\\x99\\x82, and I could use it to represent \"me\" in sentences. For example, \\xf0\\x9f\\x99\\x82\\xf0\\x9f\\x9a\\xb6\\xe2\\x80\\x8d\\xe2\\x99\\x80\\xef\\xb8\\x8f\\xf0\\x9f\\x8f\\x83\\xe2\\x80\\x8d\\xe2\\x99\\x82\\xef\\xb8\\x8f could mean \"I\\'m running to work.\" '"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Spelling Check"
      ],
      "metadata": {
        "id": "kb9jE4FaeEVJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "incorrect_text = 'ceertain conditionas during seveal ggenerations aree mmodified in the saame maner.'"
      ],
      "metadata": {
        "id": "crr1KSmudxBK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from textblob import TextBlob\n",
        "textblb = TextBlob(incorrect_text)\n",
        "textblb.correct()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SqHnoslEeLUT",
        "outputId": "53555af0-bbaa-4dfc-f035-4a4efeb5a477"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TextBlob(\"certain conditions during several generations are modified in the same manner.\")"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization"
      ],
      "metadata": {
        "id": "PCMxWVZve5y_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = '''\n",
        "          In the past two decades, there has been a significant shift in naval missions toward operations other than war.\n",
        "          Maritime security operations such as counter-piracy, maritime interdiction, maritime patrol, and naval escort are the main focus of most fleets today; however, the vessels that are currently being used in such operations were mainly built for other purposes.\n",
        "          For instance, in August 2009, the North Atlantic Council approved “Operation Ocean Shield” to fight piracy in the Gulf of Aden.\n",
        "          Among ships that were assigned in the rotations of this NATO mission, many were destroyers and frigates.\n",
        "          Although those warships can be used in such missions, how reasonable is it to risk a destroyer or a frigate to fight with terrorist boats or pirates?\n",
        "          Capable Warships vs. Smaller Combatants\n",
        "          Many surface vessels that perform maritime security operations, as in the NATO Task Force example, are sophisticated warships capable of anti-surface warfare (ASUW), anti-air warfare (AAW), and anti-submarine warfare (ASW).\n",
        "          Although these sophisticated multi-mission capable fleets are able to achieve good results in expeditionary warfare against a strong enemy [1], the capabilities of those ships will probably be used in less than 1% of their total lifetime.\n",
        "          It seems a sound reason to build capable ships in case of a conventional war, and one can claim that capable ships are built to be used in that small period of their lifetime; nevertheless, navies should optimize their efforts and resources in some way to find a better mix of vessel types and systems that constitute the vessels.\n",
        "      '''"
      ],
      "metadata": {
        "id": "GI2Hb3SufE0X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "\n",
        "sentences = sent_tokenize(text)\n",
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uTweKzrkeSty",
        "outputId": "bef57336-d701-4723-f312-5ed929019926"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\n          In the past two decades, there has been a significant shift in naval missions toward operations other than war.',\n",
              " 'Maritime security operations such as counter-piracy, maritime interdiction, maritime patrol, and naval escort are the main focus of most fleets today; however, the vessels that are currently being used in such operations were mainly built for other purposes.',\n",
              " 'For instance, in August 2009, the North Atlantic Council approved “Operation Ocean Shield” to fight piracy in the Gulf of Aden.',\n",
              " 'Among ships that were assigned in the rotations of this NATO mission, many were destroyers and frigates.',\n",
              " 'Although those warships can be used in such missions, how reasonable is it to risk a destroyer or a frigate to fight with terrorist boats or pirates?',\n",
              " 'Capable Warships vs.',\n",
              " 'Smaller Combatants\\n          Many surface vessels that perform maritime security operations, as in the NATO Task Force example, are sophisticated warships capable of anti-surface warfare (ASUW), anti-air warfare (AAW), and anti-submarine warfare (ASW).',\n",
              " 'Although these sophisticated multi-mission capable fleets are able to achieve good results in expeditionary warfare against a strong enemy [1], the capabilities of those ships will probably be used in less than 1% of their total lifetime.',\n",
              " 'It seems a sound reason to build capable ships in case of a conventional war, and one can claim that capable ships are built to be used in that small period of their lifetime; nevertheless, navies should optimize their efforts and resources in some way to find a better mix of vessel types and systems that constitute the vessels.']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for sentence in sentences:\n",
        "  print(word_tokenize(sentence))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4Daix2VfoM_",
        "outputId": "9bb456be-9239-4b00-b138-304f8f74e426"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['In', 'the', 'past', 'two', 'decades', ',', 'there', 'has', 'been', 'a', 'significant', 'shift', 'in', 'naval', 'missions', 'toward', 'operations', 'other', 'than', 'war', '.']\n",
            "['Maritime', 'security', 'operations', 'such', 'as', 'counter-piracy', ',', 'maritime', 'interdiction', ',', 'maritime', 'patrol', ',', 'and', 'naval', 'escort', 'are', 'the', 'main', 'focus', 'of', 'most', 'fleets', 'today', ';', 'however', ',', 'the', 'vessels', 'that', 'are', 'currently', 'being', 'used', 'in', 'such', 'operations', 'were', 'mainly', 'built', 'for', 'other', 'purposes', '.']\n",
            "['For', 'instance', ',', 'in', 'August', '2009', ',', 'the', 'North', 'Atlantic', 'Council', 'approved', '“', 'Operation', 'Ocean', 'Shield', '”', 'to', 'fight', 'piracy', 'in', 'the', 'Gulf', 'of', 'Aden', '.']\n",
            "['Among', 'ships', 'that', 'were', 'assigned', 'in', 'the', 'rotations', 'of', 'this', 'NATO', 'mission', ',', 'many', 'were', 'destroyers', 'and', 'frigates', '.']\n",
            "['Although', 'those', 'warships', 'can', 'be', 'used', 'in', 'such', 'missions', ',', 'how', 'reasonable', 'is', 'it', 'to', 'risk', 'a', 'destroyer', 'or', 'a', 'frigate', 'to', 'fight', 'with', 'terrorist', 'boats', 'or', 'pirates', '?']\n",
            "['Capable', 'Warships', 'vs', '.']\n",
            "['Smaller', 'Combatants', 'Many', 'surface', 'vessels', 'that', 'perform', 'maritime', 'security', 'operations', ',', 'as', 'in', 'the', 'NATO', 'Task', 'Force', 'example', ',', 'are', 'sophisticated', 'warships', 'capable', 'of', 'anti-surface', 'warfare', '(', 'ASUW', ')', ',', 'anti-air', 'warfare', '(', 'AAW', ')', ',', 'and', 'anti-submarine', 'warfare', '(', 'ASW', ')', '.']\n",
            "['Although', 'these', 'sophisticated', 'multi-mission', 'capable', 'fleets', 'are', 'able', 'to', 'achieve', 'good', 'results', 'in', 'expeditionary', 'warfare', 'against', 'a', 'strong', 'enemy', '[', '1', ']', ',', 'the', 'capabilities', 'of', 'those', 'ships', 'will', 'probably', 'be', 'used', 'in', 'less', 'than', '1', '%', 'of', 'their', 'total', 'lifetime', '.']\n",
            "['It', 'seems', 'a', 'sound', 'reason', 'to', 'build', 'capable', 'ships', 'in', 'case', 'of', 'a', 'conventional', 'war', ',', 'and', 'one', 'can', 'claim', 'that', 'capable', 'ships', 'are', 'built', 'to', 'be', 'used', 'in', 'that', 'small', 'period', 'of', 'their', 'lifetime', ';', 'nevertheless', ',', 'navies', 'should', 'optimize', 'their', 'efforts', 'and', 'resources', 'in', 'some', 'way', 'to', 'find', 'a', 'better', 'mix', 'of', 'vessel', 'types', 'and', 'systems', 'that', 'constitute', 'the', 'vessels', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Lowecasing"
      ],
      "metadata": {
        "id": "v9AdJ2-ilT6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize the article\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Convert the tokens into lowercase\n",
        "lower_tokens = [t.lower() for t in tokens]\n",
        "\n",
        "print(text)\n",
        "print(lower_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa0pAG3BlTQN",
        "outputId": "122a9e98-4f94-46a2-c8e3-7d534dc5e040"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "          In the past two decades, there has been a significant shift in naval missions toward operations other than war. \n",
            "          Maritime security operations such as counter-piracy, maritime interdiction, maritime patrol, and naval escort are the main focus of most fleets today; however, the vessels that are currently being used in such operations were mainly built for other purposes. \n",
            "          For instance, in August 2009, the North Atlantic Council approved “Operation Ocean Shield” to fight piracy in the Gulf of Aden. \n",
            "          Among ships that were assigned in the rotations of this NATO mission, many were destroyers and frigates. \n",
            "          Although those warships can be used in such missions, how reasonable is it to risk a destroyer or a frigate to fight with terrorist boats or pirates?\n",
            "          Capable Warships vs. Smaller Combatants\n",
            "          Many surface vessels that perform maritime security operations, as in the NATO Task Force example, are sophisticated warships capable of anti-surface warfare (ASUW), anti-air warfare (AAW), and anti-submarine warfare (ASW). \n",
            "          Although these sophisticated multi-mission capable fleets are able to achieve good results in expeditionary warfare against a strong enemy [1], the capabilities of those ships will probably be used in less than 1% of their total lifetime. \n",
            "          It seems a sound reason to build capable ships in case of a conventional war, and one can claim that capable ships are built to be used in that small period of their lifetime; nevertheless, navies should optimize their efforts and resources in some way to find a better mix of vessel types and systems that constitute the vessels.\n",
            "      \n",
            "['in', 'the', 'past', 'two', 'decades', ',', 'there', 'has', 'been', 'a', 'significant', 'shift', 'in', 'naval', 'missions', 'toward', 'operations', 'other', 'than', 'war', '.', 'maritime', 'security', 'operations', 'such', 'as', 'counter-piracy', ',', 'maritime', 'interdiction', ',', 'maritime', 'patrol', ',', 'and', 'naval', 'escort', 'are', 'the', 'main', 'focus', 'of', 'most', 'fleets', 'today', ';', 'however', ',', 'the', 'vessels', 'that', 'are', 'currently', 'being', 'used', 'in', 'such', 'operations', 'were', 'mainly', 'built', 'for', 'other', 'purposes', '.', 'for', 'instance', ',', 'in', 'august', '2009', ',', 'the', 'north', 'atlantic', 'council', 'approved', '“', 'operation', 'ocean', 'shield', '”', 'to', 'fight', 'piracy', 'in', 'the', 'gulf', 'of', 'aden', '.', 'among', 'ships', 'that', 'were', 'assigned', 'in', 'the', 'rotations', 'of', 'this', 'nato', 'mission', ',', 'many', 'were', 'destroyers', 'and', 'frigates', '.', 'although', 'those', 'warships', 'can', 'be', 'used', 'in', 'such', 'missions', ',', 'how', 'reasonable', 'is', 'it', 'to', 'risk', 'a', 'destroyer', 'or', 'a', 'frigate', 'to', 'fight', 'with', 'terrorist', 'boats', 'or', 'pirates', '?', 'capable', 'warships', 'vs', '.', 'smaller', 'combatants', 'many', 'surface', 'vessels', 'that', 'perform', 'maritime', 'security', 'operations', ',', 'as', 'in', 'the', 'nato', 'task', 'force', 'example', ',', 'are', 'sophisticated', 'warships', 'capable', 'of', 'anti-surface', 'warfare', '(', 'asuw', ')', ',', 'anti-air', 'warfare', '(', 'aaw', ')', ',', 'and', 'anti-submarine', 'warfare', '(', 'asw', ')', '.', 'although', 'these', 'sophisticated', 'multi-mission', 'capable', 'fleets', 'are', 'able', 'to', 'achieve', 'good', 'results', 'in', 'expeditionary', 'warfare', 'against', 'a', 'strong', 'enemy', '[', '1', ']', ',', 'the', 'capabilities', 'of', 'those', 'ships', 'will', 'probably', 'be', 'used', 'in', 'less', 'than', '1', '%', 'of', 'their', 'total', 'lifetime', '.', 'it', 'seems', 'a', 'sound', 'reason', 'to', 'build', 'capable', 'ships', 'in', 'case', 'of', 'a', 'conventional', 'war', ',', 'and', 'one', 'can', 'claim', 'that', 'capable', 'ships', 'are', 'built', 'to', 'be', 'used', 'in', 'that', 'small', 'period', 'of', 'their', 'lifetime', ';', 'nevertheless', ',', 'navies', 'should', 'optimize', 'their', 'efforts', 'and', 'resources', 'in', 'some', 'way', 'to', 'find', 'a', 'better', 'mix', 'of', 'vessel', 'types', 'and', 'systems', 'that', 'constitute', 'the', 'vessels', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Lemmatization And Stemming In Natural Language Processing\n",
        ">Languages we speak and write are made up of several words often derived from one another. When a language contains words that are derived from another word as their use in the speech changes is called **Inflected Language**\n",
        "\n",
        "> ####  \"In grammar, inflection is the modification of a word to express different grammatical categories such as tense, case, voice, aspect, person, number, gender, and mood. An inflection expresses one or more grammatical categories with a prefix, suffix or infix, or another internal modification such as a vowel change\" [Wikipedia]\n",
        "\n",
        ">The degree of inflection may be higher or lower in a language. As you have read the definition of inflection with respect to grammar, you can understand that an inflected word(s) will have a common root form. Let's look at a few examples,\n",
        "\n",
        "![img](https://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1539984207/stemminglemmatization_n8bmou.jpg)\n",
        "\n",
        ">Above examples helps us to understand the concept of normalization of text, although normalization of text is not restricted to only written document but to speech as well.\n",
        "\n",
        "The Normalization in this context can be of two Types\n",
        "* Stemming\n",
        "* Lemmatization\n",
        "\n",
        ">Stemming and Lemmatization helps us to achieve the root forms (sometimes called synonyms in search context) of inflected (derived) words.Stemming is different to Lemmatization in the approach it uses to produce root forms of words and the word produced. We will Learn more about this below.\n",
        "\n",
        "## Stemming\n",
        "\n",
        "> **\"Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to the same stem even if the stem itself is not a valid word in the Language.\"**\n",
        "\n",
        "> Stem (root) is the part of the word to which you add inflectional (changing/deriving) affixes such as (-ed,-ize, -s,-de,mis). So stemming a word or sentence may result in words that are not actual words. Stems are created by removing the suffixes or prefixes used with a word.\n",
        "\n",
        ">There are various Stemming Algorithms / methods in the NLTK library, These methods can be seen in the following diagram.\n",
        "![img](https://www.tutorialspoint.com/natural_language_toolkit/images/stemming_algorithms.jpg)\n",
        "\n",
        "* Porter stemmer: *This stemming algorithm is an older one. It’s from the 1980s and its main concern is removing the common endings to words so that they can be resolved to a common form*\n",
        "* Snowball stemmer: This algorithm is also known as the Porter2 stemming algorithm. It is almost universally accepted as better than the Porter stemmer, even being acknowledged as such by the individual who created the Porter stemmer.\n",
        "* Lancaster stemmer: Just for fun, the Lancaster stemming algorithm is another algorithm that you can use. This one is the most aggressive stemming algorithm of the bunch.\n",
        "* Regular Expression stemm: It basically takes a single regular expression and removes any prefix or suffix that matches the expression.\n",
        "\n",
        "Some basic Examples of Stemming-\n",
        "![img](https://encrypted-tbn0.gstatic.com/images?q=tbn%3AANd9GcQB544U1IwwokOrtUpO3iOx4riHTzSXnChPWg&usqp=CAU)\n",
        "\n",
        "# Lemmatization\n",
        "#### \"Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language. In Lemmatization root word is called Lemma. A lemma (plural lemmas or lemmata) is the canonical form, dictionary form, or citation form of a set of words.\"\n",
        "\n",
        ">***For example, runs, running, ran are all forms of the word run, therefore run is the lemma of all these words. Because lemmatization returns an actual word of the language, it is used where it is necessary to get valid words.***\n",
        "\n",
        "Some Examples of Lemmatization are as follows -\n",
        "\n",
        "![img](https://kavita-ganesan.com/wp-content/uploads/2019/02/Screen-Shot-2019-02-20-at-4.49.08-PM.png)\n",
        "\n",
        "#### *****The Major Point to Note is That each word that is Lemmatized belongs to a language unlike Stemming.*****\n",
        "\n",
        "# Lemmatization Vs Stemming\n",
        "> After Reading the full article we can derive the following inferences about the difference between Lemmatization and stemming.\n",
        "* Stemming and Lemmatization both generate the root form of the inflected words. The difference is that stem might not be an actual word whereas, lemma is an actual language word.\n",
        "* Stemming follows an algorithm with steps to perform on the words which makes it faster. Whereas, in lemmatization, we use WordNet corpus and a corpus for stop words as well to produce lemma which makes it slower than stemming.\n",
        "\n",
        "![img](https://miro.medium.com/max/2050/1*ES5bt7IoInIq2YioQp2zcQ.png)\n",
        "\n",
        "> ****We can see how the meaning of the word is conserved in Lemmatization as compared to Stemming****"
      ],
      "metadata": {
        "id": "lf3yT39SgKqa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming"
      ],
      "metadata": {
        "id": "FhWqMlYbjc5s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words=[\"eating\",\"eats\",\"eaten\",\"writing\",\"writes\",\"programming\",\"programs\",\"history\",\"finally\",\"finalized\"]"
      ],
      "metadata": {
        "id": "414OkcG2f4t2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### PorterStemmer"
      ],
      "metadata": {
        "id": "r6_uQfKQjkxX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "stemming=PorterStemmer()"
      ],
      "metadata": {
        "id": "Z_kWWqDYjqrO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word+\"---->\"+stemming.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9FvaWnvijnOS",
        "outputId": "c7e3df72-bd91-447a-897f-21b244b536ee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eaten\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->histori\n",
            "finally---->final\n",
            "finalized---->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem('congratulations')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "8cXsSaSNjtJS",
        "outputId": "fbe00525-40fe-4285-b1db-4207b206ec43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'congratul'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RegexpStemmer class\n",
        "NLTK has RegexpStemmer class with the help of which we can easily implement Regular Expression Stemmer algorithms. It basically takes a single regular expression and removes any prefix or suffix that matches the expression. Let us see an example"
      ],
      "metadata": {
        "id": "7KFhE2sIj0Yi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import RegexpStemmer\n",
        "\n",
        "reg_stemmer=RegexpStemmer('ing$|s$|e$|able$', min=4)"
      ],
      "metadata": {
        "id": "A5SVZdTfkTzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem('eating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "SPFGbnwqjxpG",
        "outputId": "8a767fbe-b2ad-4353-8d85-4ae66204e0b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'eat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "reg_stemmer.stem('ingeating')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "73BK4fbVkV-i",
        "outputId": "f865347b-9746-4586-bf67-bcbceeb4fe59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'ingeat'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Snowball Stemmer\n",
        "It is a stemming algorithm which is also known as the Porter2 stemming algorithm as it is a better version of the Porter Stemmer since some issues of it were fixed in this stemmer."
      ],
      "metadata": {
        "id": "cr1ZgbxdkbmP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "snowballsstemmer=SnowballStemmer('english')"
      ],
      "metadata": {
        "id": "8AZze6TNkbKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word+\"---->\"+snowballsstemmer.stem(word))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2-EzFaRkYf7",
        "outputId": "645564e0-e8c8-4af6-ab35-ac24f00a5360"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eaten\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->histori\n",
            "finally---->final\n",
            "finalized---->final\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemming.stem(\"fairly\"),stemming.stem(\"sportingly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1vwuUJm2kkMs",
        "outputId": "94fcac83-1bb4-40ec-fd9b-f56d7856848e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fairli', 'sportingli')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "snowballsstemmer.stem(\"fairly\"),snowballsstemmer.stem(\"sportingly\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vwup7g8QkmYb",
        "outputId": "bd4d80d7-16d9-4f6e-e87d-644f6c639d51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('fair', 'sport')"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Wordnet Lemmatizer\n",
        "Lemmatization technique is like stemming. The output we will get after lemmatization is called ‘lemma’, which is a root word rather than root stem, the output of stemming. After lemmatization, we will be getting a valid word that means the same thing.\n",
        "\n",
        "NLTK provides WordNetLemmatizer class which is a thin wrapper around the wordnet corpus. This class uses morphy() function to the WordNet CorpusReader class to find a lemma. Let us understand it with an example −"
      ],
      "metadata": {
        "id": "ghtVZe2ml7ED"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "import nltk\n",
        "nltk.download('wordnet')\n",
        "\n",
        "lemmatizer=WordNetLemmatizer()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwQL7CAWmER0",
        "outputId": "2ad22388-00c7-45b5-f770-c31e71f4cad8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "POS- Noun-n\n",
        "verb-v\n",
        "adjective-a\n",
        "adverb-r\n",
        "'''\n",
        "lemmatizer.lemmatize(\"going\",pos='v')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PM8-ogzQkqq4",
        "outputId": "44c2eb7c-b2ed-467c-adf3-fe8c4159d5c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'go'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "    print(word+\"---->\"+lemmatizer.lemmatize(word,pos='v'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66qsDDC3mGDM",
        "outputId": "99f40938-53fd-461a-e3c2-03e1f402d0a4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eating---->eat\n",
            "eats---->eat\n",
            "eaten---->eat\n",
            "writing---->write\n",
            "writes---->write\n",
            "programming---->program\n",
            "programs---->program\n",
            "history---->history\n",
            "finally---->finally\n",
            "finalized---->finalize\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing Stopword"
      ],
      "metadata": {
        "id": "qVwN5p7Dwnsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0Z408whwmmr",
        "outputId": "8e23682d-158c-426a-cb8e-92567007bba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "75CNQ5zMmuEj",
        "outputId": "6d402e1c-d347-4840-f96b-bffdad3a9555"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['i',\n",
              " 'me',\n",
              " 'my',\n",
              " 'myself',\n",
              " 'we',\n",
              " 'our',\n",
              " 'ours',\n",
              " 'ourselves',\n",
              " 'you',\n",
              " \"you're\",\n",
              " \"you've\",\n",
              " \"you'll\",\n",
              " \"you'd\",\n",
              " 'your',\n",
              " 'yours',\n",
              " 'yourself',\n",
              " 'yourselves',\n",
              " 'he',\n",
              " 'him',\n",
              " 'his',\n",
              " 'himself',\n",
              " 'she',\n",
              " \"she's\",\n",
              " 'her',\n",
              " 'hers',\n",
              " 'herself',\n",
              " 'it',\n",
              " \"it's\",\n",
              " 'its',\n",
              " 'itself',\n",
              " 'they',\n",
              " 'them',\n",
              " 'their',\n",
              " 'theirs',\n",
              " 'themselves',\n",
              " 'what',\n",
              " 'which',\n",
              " 'who',\n",
              " 'whom',\n",
              " 'this',\n",
              " 'that',\n",
              " \"that'll\",\n",
              " 'these',\n",
              " 'those',\n",
              " 'am',\n",
              " 'is',\n",
              " 'are',\n",
              " 'was',\n",
              " 'were',\n",
              " 'be',\n",
              " 'been',\n",
              " 'being',\n",
              " 'have',\n",
              " 'has',\n",
              " 'had',\n",
              " 'having',\n",
              " 'do',\n",
              " 'does',\n",
              " 'did',\n",
              " 'doing',\n",
              " 'a',\n",
              " 'an',\n",
              " 'the',\n",
              " 'and',\n",
              " 'but',\n",
              " 'if',\n",
              " 'or',\n",
              " 'because',\n",
              " 'as',\n",
              " 'until',\n",
              " 'while',\n",
              " 'of',\n",
              " 'at',\n",
              " 'by',\n",
              " 'for',\n",
              " 'with',\n",
              " 'about',\n",
              " 'against',\n",
              " 'between',\n",
              " 'into',\n",
              " 'through',\n",
              " 'during',\n",
              " 'before',\n",
              " 'after',\n",
              " 'above',\n",
              " 'below',\n",
              " 'to',\n",
              " 'from',\n",
              " 'up',\n",
              " 'down',\n",
              " 'in',\n",
              " 'out',\n",
              " 'on',\n",
              " 'off',\n",
              " 'over',\n",
              " 'under',\n",
              " 'again',\n",
              " 'further',\n",
              " 'then',\n",
              " 'once',\n",
              " 'here',\n",
              " 'there',\n",
              " 'when',\n",
              " 'where',\n",
              " 'why',\n",
              " 'how',\n",
              " 'all',\n",
              " 'any',\n",
              " 'both',\n",
              " 'each',\n",
              " 'few',\n",
              " 'more',\n",
              " 'most',\n",
              " 'other',\n",
              " 'some',\n",
              " 'such',\n",
              " 'no',\n",
              " 'nor',\n",
              " 'not',\n",
              " 'only',\n",
              " 'own',\n",
              " 'same',\n",
              " 'so',\n",
              " 'than',\n",
              " 'too',\n",
              " 'very',\n",
              " 's',\n",
              " 't',\n",
              " 'can',\n",
              " 'will',\n",
              " 'just',\n",
              " 'don',\n",
              " \"don't\",\n",
              " 'should',\n",
              " \"should've\",\n",
              " 'now',\n",
              " 'd',\n",
              " 'll',\n",
              " 'm',\n",
              " 'o',\n",
              " 're',\n",
              " 've',\n",
              " 'y',\n",
              " 'ain',\n",
              " 'aren',\n",
              " \"aren't\",\n",
              " 'couldn',\n",
              " \"couldn't\",\n",
              " 'didn',\n",
              " \"didn't\",\n",
              " 'doesn',\n",
              " \"doesn't\",\n",
              " 'hadn',\n",
              " \"hadn't\",\n",
              " 'hasn',\n",
              " \"hasn't\",\n",
              " 'haven',\n",
              " \"haven't\",\n",
              " 'isn',\n",
              " \"isn't\",\n",
              " 'ma',\n",
              " 'mightn',\n",
              " \"mightn't\",\n",
              " 'mustn',\n",
              " \"mustn't\",\n",
              " 'needn',\n",
              " \"needn't\",\n",
              " 'shan',\n",
              " \"shan't\",\n",
              " 'shouldn',\n",
              " \"shouldn't\",\n",
              " 'wasn',\n",
              " \"wasn't\",\n",
              " 'weren',\n",
              " \"weren't\",\n",
              " 'won',\n",
              " \"won't\",\n",
              " 'wouldn',\n",
              " \"wouldn't\"]"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stemmer=PorterStemmer()\n",
        "\n",
        "sentences=nltk.sent_tokenize(text)\n",
        "\n",
        "for i in range(len(sentences)):\n",
        "    words=nltk.word_tokenize(sentences[i])\n",
        "    words=[stemmer.stem(word) for word in words if word not in set(stopwords.words('english'))]\n",
        "    sentences[i]=' '.join(words)"
      ],
      "metadata": {
        "id": "Ny_cPLhvwwMo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-l5DRiQwxG80",
        "outputId": "c6d510a0-eefe-4d94-80ef-98080da77779"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['in past two decad , signific shift naval mission toward oper war .',\n",
              " 'maritim secur oper counter-piraci , maritim interdict , maritim patrol , naval escort main focu fleet today ; howev , vessel current use oper mainli built purpos .',\n",
              " 'for instanc , august 2009 , north atlant council approv “ oper ocean shield ” fight piraci gulf aden .',\n",
              " 'among ship assign rotat nato mission , mani destroy frigat .',\n",
              " 'although warship use mission , reason risk destroy frigat fight terrorist boat pirat ?',\n",
              " 'capabl warship vs .',\n",
              " 'smaller combat mani surfac vessel perform maritim secur oper , nato task forc exampl , sophist warship capabl anti-surfac warfar ( asuw ) , anti-air warfar ( aaw ) , anti-submarin warfar ( asw ) .',\n",
              " 'although sophist multi-miss capabl fleet abl achiev good result expeditionari warfar strong enemi [ 1 ] , capabl ship probabl use less 1 % total lifetim .',\n",
              " 'it seem sound reason build capabl ship case convent war , one claim capabl ship built use small period lifetim ; nevertheless , navi optim effort resourc way find better mix vessel type system constitut vessel .']"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Parts of Speech Tagging"
      ],
      "metadata": {
        "id": "x5-1l8KRxags"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wSMt9O--xJn1",
        "outputId": "72213bb8-b164-446e-9b9d-b2af3b005b56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(sentences)):\n",
        "    words=nltk.word_tokenize(sentences[i])\n",
        "    words=[word for word in words if word not in set(stopwords.words('english'))]\n",
        "    #sentences[i]=' '.join(words)# converting all the list of words into sentences\n",
        "    pos_tag=nltk.pos_tag(words)\n",
        "    print(pos_tag)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wsdhajaBxdh_",
        "outputId": "03abbfcb-a233-49e1-e512-892a8b352034"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('past', 'JJ'), ('two', 'CD'), ('decad', 'NN'), (',', ','), ('signific', 'JJ'), ('shift', 'NN'), ('naval', 'JJ'), ('mission', 'NN'), ('toward', 'IN'), ('oper', 'JJR'), ('war', 'NN'), ('.', '.')]\n",
            "[('maritim', 'NN'), ('secur', 'JJ'), ('oper', 'IN'), ('counter-piraci', 'NN'), (',', ','), ('maritim', 'NN'), ('interdict', 'NN'), (',', ','), ('maritim', 'NN'), ('patrol', 'NN'), (',', ','), ('naval', 'JJ'), ('escort', 'NN'), ('main', 'JJ'), ('focu', 'NN'), ('fleet', 'NN'), ('today', 'NN'), (';', ':'), ('howev', 'NN'), (',', ','), ('vessel', 'FW'), ('current', 'JJ'), ('use', 'NN'), ('oper', 'NN'), ('mainli', 'NN'), ('built', 'VBN'), ('purpos', 'NN'), ('.', '.')]\n",
            "[('instanc', 'NN'), (',', ','), ('august', 'JJ'), ('2009', 'CD'), (',', ','), ('north', 'JJ'), ('atlant', 'JJ'), ('council', 'NN'), ('approv', 'NN'), ('“', 'NNP'), ('oper', 'IN'), ('ocean', 'JJ'), ('shield', 'NN'), ('”', 'NNP'), ('fight', 'NN'), ('piraci', 'NN'), ('gulf', 'NN'), ('aden', 'NN'), ('.', '.')]\n",
            "[('among', 'IN'), ('ship', 'JJ'), ('assign', 'JJ'), ('rotat', 'NN'), ('nato', 'JJ'), ('mission', 'NN'), (',', ','), ('mani', 'FW'), ('destroy', 'NN'), ('frigat', 'NN'), ('.', '.')]\n",
            "[('although', 'IN'), ('warship', 'NN'), ('use', 'NN'), ('mission', 'NN'), (',', ','), ('reason', 'NN'), ('risk', 'NN'), ('destroy', 'NN'), ('frigat', 'JJ'), ('fight', 'JJ'), ('terrorist', 'NN'), ('boat', 'NN'), ('pirat', 'NN'), ('?', '.')]\n",
            "[('capabl', 'NN'), ('warship', 'NN'), ('vs', 'NN'), ('.', '.')]\n",
            "[('smaller', 'JJR'), ('combat', 'NN'), ('mani', 'NN'), ('surfac', 'JJ'), ('vessel', 'NN'), ('perform', 'NN'), ('maritim', 'NN'), ('secur', 'NN'), ('oper', 'IN'), (',', ','), ('nato', 'FW'), ('task', 'NN'), ('forc', 'NN'), ('exampl', 'NN'), (',', ','), ('sophist', 'JJ'), ('warship', 'NN'), ('capabl', 'VBD'), ('anti-surfac', 'JJ'), ('warfar', 'NN'), ('(', '('), ('asuw', 'NN'), (')', ')'), (',', ','), ('anti-air', 'JJ'), ('warfar', 'NN'), ('(', '('), ('aaw', 'NN'), (')', ')'), (',', ','), ('anti-submarin', 'JJ'), ('warfar', 'NN'), ('(', '('), ('asw', 'NN'), (')', ')'), ('.', '.')]\n",
            "[('although', 'IN'), ('sophist', 'JJ'), ('multi-miss', 'JJ'), ('capabl', 'NN'), ('fleet', 'NN'), ('abl', 'IN'), ('achiev', 'RB'), ('good', 'JJ'), ('result', 'NN'), ('expeditionari', 'NN'), ('warfar', 'NN'), ('strong', 'JJ'), ('enemi', 'NN'), ('[', '$'), ('1', 'CD'), (']', 'NNP'), (',', ','), ('capabl', 'NN'), ('ship', 'NN'), ('probabl', 'NN'), ('use', 'NN'), ('less', 'CC'), ('1', 'CD'), ('%', 'NN'), ('total', 'JJ'), ('lifetim', 'NN'), ('.', '.')]\n",
            "[('seem', 'VBP'), ('sound', 'JJ'), ('reason', 'NN'), ('build', 'VB'), ('capabl', 'NN'), ('ship', 'NN'), ('case', 'NN'), ('convent', 'JJ'), ('war', 'NN'), (',', ','), ('one', 'CD'), ('claim', 'NN'), ('capabl', 'NN'), ('ship', 'NN'), ('built', 'VBN'), ('use', 'IN'), ('small', 'JJ'), ('period', 'NN'), ('lifetim', 'NN'), (';', ':'), ('nevertheless', 'RB'), (',', ','), ('navi', 'JJ'), ('optim', 'NN'), ('effort', 'NN'), ('resourc', 'VBZ'), ('way', 'NN'), ('find', 'VB'), ('better', 'JJR'), ('mix', 'NN'), ('vessel', 'NN'), ('type', 'NN'), ('system', 'NN'), ('constitut', 'JJ'), ('vessel', 'NN'), ('.', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(nltk.pos_tag(\"Taj Mahal is a beautiful Monument\".split()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFEw2Gtpxf0N",
        "outputId": "239a559b-2b0a-4492-d144-b88a72ab7fe6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('Taj', 'NNP'), ('Mahal', 'NNP'), ('is', 'VBZ'), ('a', 'DT'), ('beautiful', 'JJ'), ('Monument', 'NN')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk import word_tokenize, pos_tag, ne_chunk\n",
        "\n",
        "# Download required NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "\n",
        "# Sample text\n",
        "text = \"Apple Inc. is an American multinational technology company headquartered in Cupertino, California, that designs, develops, and sells consumer electronics, computer software, and online services. Microsoft Corporation is an American multinational technology corporation which produces computer software, consumer electronics, personal computers, and related services.\"\n",
        "\n",
        "# Tokenize the text\n",
        "tokens = word_tokenize(text)\n",
        "\n",
        "# Perform POS tagging\n",
        "tagged = pos_tag(tokens)\n",
        "\n",
        "# Named Entity Recognition\n",
        "entities = ne_chunk(tagged)\n",
        "\n",
        "# Print the entities\n",
        "print(entities)"
      ],
      "metadata": {
        "id": "18mAL6Y2xi7A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "9s3wsvaIyZbO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TgKAyJg7yZJe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}